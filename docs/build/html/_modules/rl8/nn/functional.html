

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>rl8.nn.functional &mdash; rl8  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css" />

  
      <script src="../../../_static/jquery.js"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
      <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
      <script src="../../../_static/doctools.js"></script>
      <script src="../../../_static/sphinx_highlight.js"></script>
      <script src="../../../_static/clipboard.min.js"></script>
      <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            rl8
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/modules.html">API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">rl8</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">rl8.nn.functional</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for rl8.nn.functional</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Functional PyTorch definitions.&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensordict</span><span class="w"> </span><span class="kn">import</span> <span class="n">TensorDict</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">..data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataKeys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">..distributions</span><span class="w"> </span><span class="kn">import</span> <span class="n">Distribution</span>

<span class="n">FINFO</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">()</span>


<div class="viewcode-block" id="binary_mask_to_float_mask"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.binary_mask_to_float_mask">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">binary_mask_to_float_mask</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">/</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert ``0`` and ``1`` elements in a binary mask to ``-inf`` and ``0``,</span>
<span class="sd">    respectively.</span>

<span class="sd">    Args:</span>
<span class="sd">        mask: Binary mask tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Float mask tensor where ``0`` indicates an UNPADDED or VALID value.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span>
        <span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="float_mask_to_binary_mask"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.float_mask_to_binary_mask">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">float_mask_to_binary_mask</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">/</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert ``0`` and ``-inf`` elements into a boolean mask of ``True`` and</span>
<span class="sd">    ``False``, respectively.</span>

<span class="sd">    Args:</span>
<span class="sd">        mask: Float mask tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Boolean mask tensor where ``True`` indicates an UNPADDED or VALID value.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">),</span> <span class="kc">False</span><span class="p">)</span>
        <span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="o">.</span><span class="n">bool</span><span class="p">()</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="generalized_advantage_estimate"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.generalized_advantage_estimate">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">generalized_advantage_estimate</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">TensorDict</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">gae_lambda</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.95</span><span class="p">,</span>
    <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">normalize_advantages</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">return_returns</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">reward_scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute a Generalized Advantage Estimate (GAE) and, optionally,</span>
<span class="sd">    returns using value function estimates and rewards.</span>

<span class="sd">    GAE is most commonly used with PPO for computing a policy loss that</span>
<span class="sd">    incentivizes &quot;good&quot; actions.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch: Tensordict of batch size ``[B, T + 1, ...]`` that contains the</span>
<span class="sd">            following keys:</span>

<span class="sd">            - &quot;rewards&quot;: Environment transition rewards.</span>
<span class="sd">            - &quot;values&quot;: Policy value function estimates.</span>

<span class="sd">        gae_lambda: Generalized Advantage Estimation (GAE) hyperparameter for</span>
<span class="sd">            controlling the variance and bias tradeoff when estimating the</span>
<span class="sd">            state value function from collected environment transitions. A</span>
<span class="sd">            higher value allows higher variance while a lower value allows</span>
<span class="sd">            higher bias estimation but lower variance.</span>
<span class="sd">        gamma: Discount reward factor often used in the Bellman operator for</span>
<span class="sd">            controlling the variance and bias tradeoff in collected experienced</span>
<span class="sd">            rewards. Note, this does not control the bias/variance of the</span>
<span class="sd">            state value estimation and only controls the weight future rewards</span>
<span class="sd">            have on the total discounted return.</span>
<span class="sd">        inplace: Whether to store advantage and, optionally, return estimates</span>
<span class="sd">            in the given tensordict or whether to allocate a separate tensordict</span>
<span class="sd">            for the returned values.</span>
<span class="sd">        normalize_advantages: Whether to normalize advantages using the mean and</span>
<span class="sd">            standard deviation of the advantage batch before storing in the</span>
<span class="sd">            returned tensordict.</span>
<span class="sd">        return_returns: Whether to compute and return Monte Carlo return</span>
<span class="sd">            estimates with GAE.</span>
<span class="sd">        reward_scale: Reward scale to use; useful for normalizing rewards</span>
<span class="sd">            for stabilizing learning and improving overall performance.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensordict with at least advantages and, optionally, discounted</span>
<span class="sd">        returns.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">TensorDict</span><span class="p">({},</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">batch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">out</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">out</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">REWARDS</span><span class="p">])</span>
    <span class="n">batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">REWARDS</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">REWARDS</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">reward_scale</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
    <span class="n">prev_advantage</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">REWARDS</span><span class="p">][:,</span> <span class="n">t</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span>
            <span class="n">gamma</span> <span class="o">*</span> <span class="n">batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">VALUES</span><span class="p">][:,</span> <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
            <span class="o">-</span> <span class="n">batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">VALUES</span><span class="p">][:,</span> <span class="n">t</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">][:,</span> <span class="n">t</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_advantage</span> <span class="o">=</span> <span class="n">delta</span> <span class="o">+</span> <span class="p">(</span>
            <span class="n">gamma</span> <span class="o">*</span> <span class="n">gae_lambda</span> <span class="o">*</span> <span class="n">prev_advantage</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">return_returns</span><span class="p">:</span>
        <span class="n">out</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">RETURNS</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">]</span> <span class="o">+</span> <span class="n">batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">VALUES</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">normalize_advantages</span><span class="p">:</span>
        <span class="n">std</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">std_mean</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">][:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>
        <span class="n">out</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">][:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">out</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">][:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span>
        <span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="mask_from_lengths"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.mask_from_lengths">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">mask_from_lengths</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">/</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Return sequence mask that indicates UNPADDED or VALID values</span>
<span class="sd">    according to tensor lengths.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Tensor with shape ``[B, T, ...]``.</span>
<span class="sd">        lengths: Tensor with shape ``[B]`` that indicates lengths of the</span>
<span class="sd">            ``T`` sequence for each B element in ``x``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sequence mask of shape ``[B, T]``.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">range_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">range_tensor</span> <span class="o">&lt;</span> <span class="n">lengths</span></div>


<div class="viewcode-block" id="masked_avg"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.masked_avg">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">masked_avg</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">keepdim</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply a masked average to ``x`` along ``dim``.</span>

<span class="sd">    Useful for pooling potentially padded features.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Tensor with shape ``[B, T, ...]`` to apply pooling to.</span>
<span class="sd">        mask: Mask with shape ``[B, T]`` indicating UNPADDED or VALID values.</span>
<span class="sd">        dim: Dimension to pool along.</span>
<span class="sd">        keepdim: Whether to keep the pooled dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Masked max of ``x`` along ``dim`` and the indices of those maximums.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">while</span> <span class="n">mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">():</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">masksum</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">x</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">masksum</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">keepdim</span><span class="p">:</span>
        <span class="n">avg</span> <span class="o">=</span> <span class="n">avg</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg</span></div>


<div class="viewcode-block" id="masked_categorical_sample"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.masked_categorical_sample">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">masked_categorical_sample</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">/</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Masked categorical sampling of ``x``.</span>

<span class="sd">    Typically used for sampling from outputs of :meth:`masked_log_softmax`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Logits with shape ``[B, T, ...]`` to sample from.</span>
<span class="sd">        mask: Mask with shape ``[B, T]`` indicating UNPADDED or VALID values.</span>
<span class="sd">        dim: Dimension to gather sampled values along.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Sampled logits and the indices of those sampled logits.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">while</span> <span class="n">mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">():</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mask</span><span class="p">),</span> <span class="n">FINFO</span><span class="o">.</span><span class="n">min</span><span class="p">,</span> <span class="n">FINFO</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
    <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">samples</span><span class="p">),</span> <span class="n">samples</span></div>


<div class="viewcode-block" id="masked_log_softmax"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.masked_log_softmax">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">masked_log_softmax</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">/</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply a masked log softmax to ``x`` along ``dim``.</span>

<span class="sd">    Typically used for getting logits from a model that predicts a sequence.</span>
<span class="sd">    The output of this function is typically passed to :meth:`masked_categorical_sample`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Tensor with shape ``[B, T, ...]``.</span>
<span class="sd">        mask: Mask with shape ``[B, T]`` indicating UNPADDED or VALID values.</span>
<span class="sd">        dim: Dimension to apply log softmax along.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Logits.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">while</span> <span class="n">mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">():</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mask</span><span class="p">),</span> <span class="n">FINFO</span><span class="o">.</span><span class="n">min</span><span class="p">,</span> <span class="n">FINFO</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="masked_max"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.masked_max">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">masked_max</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply a masked max to ``x`` along ``dim``.</span>

<span class="sd">    Useful for pooling potentially padded features.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Tensor with shape ``[B, T, ...]`` to apply pooling to.</span>
<span class="sd">        mask: Mask with shape ``[B, T]`` indicating UNPADDED or VALID values.</span>
<span class="sd">        dim: Dimension to pool along.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Masked max of ``x`` along ``dim`` and the indices of those maximums.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">while</span> <span class="n">mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">():</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="n">FINFO</span><span class="o">.</span><span class="n">min</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">idx</span><span class="p">),</span> <span class="n">idx</span></div>


<div class="viewcode-block" id="ppo_losses"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.ppo_losses">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">ppo_losses</span><span class="p">(</span>
    <span class="n">buffer_batch</span><span class="p">:</span> <span class="n">TensorDict</span><span class="p">,</span>
    <span class="n">sample_batch</span><span class="p">:</span> <span class="n">TensorDict</span><span class="p">,</span>
    <span class="n">sample_distribution</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">clip_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">dual_clip_param</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span>
    <span class="n">entropy_coeff</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">vf_clip_param</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">vf_coeff</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorDict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Proximal Policy Optimization loss.</span>

<span class="sd">    Includes a dual-clipped policy loss, value function estimate loss,</span>
<span class="sd">    and an optional entropy bonus loss. All losses are summed into a</span>
<span class="sd">    total loss and reduced with a mean operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        buffer_batch: Tensordict of batch size ``[B, ...]`` full of the</span>
<span class="sd">            following keys:</span>

<span class="sd">            - &quot;actions&quot;: Policy action samples during environment transitions.</span>
<span class="sd">            - &quot;advantages&quot;: Advantages from :meth:`generalized_advantage_estimate`.</span>
<span class="sd">            - &quot;logp&quot;: Log probabilities of taking ``&quot;actions&quot;``.</span>
<span class="sd">            - &quot;returns&quot;: Monte carlo return estimates.</span>

<span class="sd">        sample_batch: Tensordict from sampling a policy of batch size ``[B, ...]``</span>
<span class="sd">            full of the following keys:</span>

<span class="sd">            - &quot;values&quot;: Policy value function estimates.</span>

<span class="sd">        sample_distribution: A distribution instance created from the model</span>
<span class="sd">            that provided ``sample_batch`` used for computing the policy</span>
<span class="sd">            loss and entropy bonus loss.</span>
<span class="sd">        clip_param: PPO hyperparameter indicating the max distance the policy can</span>
<span class="sd">            update away from previously collected policy sample data with</span>
<span class="sd">            respect to likelihoods of taking actions conditioned on</span>
<span class="sd">            observations. This is the main innovation of PPO.</span>
<span class="sd">        dual_clip_param: PPO hyperparameter that clips like ``clip_param`` but when</span>
<span class="sd">            advantage estimations are negative. Helps prevent instability for</span>
<span class="sd">            continuous action spaces when policies are making large updates.</span>
<span class="sd">            Leave ``None`` for this clip to not apply. Otherwise, typical values</span>
<span class="sd">            are around ``5``.</span>
<span class="sd">        entropy_coeff: Entropy coefficient value. Weight of the entropy loss w.r.t.</span>
<span class="sd">            other components of total loss.</span>
<span class="sd">        vf_clip_param: PPO hyperparameter similar to ``clip_param`` but for</span>
<span class="sd">            the value function estimate. A measure of max distance the model&#39;s</span>
<span class="sd">            value function is allowed to update away from previous value</span>
<span class="sd">            function samples.</span>
<span class="sd">        vf_coeff: Value function loss component weight. Only needs to be tuned</span>
<span class="sd">            when the policy and value function share parameters.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensordict containing each of the loss components.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p_ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">sample_distribution</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">buffer_batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ACTIONS</span><span class="p">])</span>
        <span class="o">-</span> <span class="n">buffer_batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">LOGP</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">vf_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span>
                <span class="n">sample_batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">VALUES</span><span class="p">],</span>
                <span class="n">buffer_batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">RETURNS</span><span class="p">],</span>
                <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="mf">0.0</span><span class="p">,</span>
            <span class="n">vf_clip_param</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">surr1</span> <span class="o">=</span> <span class="n">buffer_batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">]</span> <span class="o">*</span> <span class="n">p_ratio</span>
    <span class="n">surr2</span> <span class="o">=</span> <span class="n">buffer_batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
        <span class="n">p_ratio</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">clip_param</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">clip_param</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">dual_clip_param</span><span class="p">:</span>
        <span class="n">clip1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">surr1</span><span class="p">,</span> <span class="n">surr2</span><span class="p">)</span>
        <span class="n">clip2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span>
            <span class="n">clip1</span><span class="p">,</span>
            <span class="n">dual_clip_param</span> <span class="o">*</span> <span class="n">buffer_batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">buffer_batch</span><span class="p">[</span><span class="n">DataKeys</span><span class="o">.</span><span class="n">ADVANTAGES</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">clip2</span><span class="p">,</span> <span class="n">clip1</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span>
            <span class="n">surr1</span><span class="p">,</span>
            <span class="n">surr2</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">vf_coeff</span> <span class="o">*</span> <span class="n">vf_loss</span> <span class="o">-</span> <span class="n">policy_loss</span>
    <span class="k">if</span> <span class="n">entropy_coeff</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">entropy_loss</span> <span class="o">=</span> <span class="n">sample_distribution</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">-=</span> <span class="n">entropy_coeff</span> <span class="o">*</span> <span class="n">entropy_loss</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">entropy_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">TensorDict</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;entropy&quot;</span><span class="p">:</span> <span class="n">entropy_loss</span><span class="p">,</span>
            <span class="s2">&quot;policy&quot;</span><span class="p">:</span> <span class="n">policy_loss</span><span class="p">,</span>
            <span class="s2">&quot;vf&quot;</span><span class="p">:</span> <span class="n">vf_loss</span><span class="p">,</span>
            <span class="s2">&quot;total&quot;</span><span class="p">:</span> <span class="n">total_loss</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="p">[],</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="skip_connection"><a class="viewcode-back" href="../../../api/rl8.nn.html#rl8.nn.functional.skip_connection">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">skip_connection</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">kind</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform a skip connection for ``x`` and ``y``.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Skip connection seed with shape ``[B, T, ...]``.</span>
<span class="sd">        y: Skip connection seed with same shape as ``x``.</span>
<span class="sd">        kind: Type of skip connection to use.</span>
<span class="sd">            Options include:</span>

<span class="sd">                - &quot;residual&quot; for a standard residual connection (summing outputs)</span>
<span class="sd">                - &quot;cat&quot; for concatenating outputs</span>
<span class="sd">                - ``None`` for no skip connection</span>

<span class="sd">        dim: Dimension to apply concatentation along. Only valid when</span>
<span class="sd">            ``kind`` is ``&quot;cat&quot;``</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor with shape depending on ``kind``.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">match</span> <span class="n">kind</span><span class="p">:</span>
        <span class="k">case</span> <span class="s2">&quot;residual&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
        <span class="k">case</span> <span class="s2">&quot;cat&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="k">case</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">y</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No skip connection type for </span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Andrew B.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>